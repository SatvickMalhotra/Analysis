{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#this code is used to load the Data file CSV into My system and storing it into dataframe\n",
        "#chunk wise loading to save memory and time windows 1932 only work here to acoid errors idk why m-swasth issue in data\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "chunk_size = 60000\n",
        "\n",
        "# Initialize an empty list to store the chunks\n",
        "processed_data = []\n",
        "\n",
        "# Read the specified rows in chunks from the CSV file\n",
        "for chunk in pd.read_csv(r\"C:\\Users\\satvi\\OneDrive\\Desktop\\m-swasth\\2024\\feb\\all_partner_recon_report\\all_partner_recon_report.csv\", sep = '|', encoding = 'windows-1252',\n",
        "                         encoding_errors='ignore',on_bad_lines ='skip', chunksize = chunk_size):\n",
        "    print(\"added chunk\")\n",
        "    processed_data.append(chunk)\n",
        "\n",
        "# Concatenate the chunks into a single DataFrame\n",
        "processed_df_1 = pd.concat(processed_data)\n",
        "headers = pd.read_csv(r\"C:\\Users\\satvi\\OneDrive\\Desktop\\m-swasth\\2024\\feb\\all_partner_recon_report\\all_partner_recon_report.csv\", nrows = 1, sep = '|')\n",
        "processed_df_1.columns = headers.columns\n",
        "print('Processing data...')"
      ],
      "metadata": {
        "id": "E_z4PPuhSlxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting them into date time format here yyyy=mm-dd\n",
        "\n",
        "# Assuming processed_df_1 is your DataFrame\n",
        "processed_df_1['Subscription Date'] = pd.to_datetime(processed_df_1['Subscription Date'])\n",
        "processed_df_1['Expiry Date'] = pd.to_datetime(processed_df_1['Expiry Date'])\n",
        "processed_df_1['Payment Date'] = pd.to_datetime(processed_df_1['Payment Date'])\n",
        "\n"
      ],
      "metadata": {
        "id": "kE6xKBTESl33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keeping only the required columns here\n",
        "columns_to_keep = ['Channel', 'Policy #', 'Member Id', 'Loan #', 'Subscription Date', 'Expiry Date', 'Branch Name', 'Branch Id', 'Payment Date']\n",
        "processed_df_1 = processed_df_1[columns_to_keep]\n"
      ],
      "metadata": {
        "id": "luQXmBINSl6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a pivot for each channel to know the count of policy\n",
        "#WAY 1\n",
        "# Assuming your DataFrame is named processed_df_1\n",
        "pivot_df = processed_df_1.groupby(['Channel', 'Policy #', 'Payment Date']).size().reset_index(name='Count')\n",
        "\n",
        "# Pivot the data\n",
        "pivot_table = pivot_df.pivot_table(index=['Channel', 'Policy #', 'Count'], values='Payment Date', aggfunc=lambda x: ', '.join(x.dt.strftime('%d %B %Y')))\n",
        "\n",
        "print(pivot_table)\n"
      ],
      "metadata": {
        "id": "UCy1xuSHSl83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WAY 2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you already have your data in a DataFrame called 'processed_df_1'\n",
        "\n",
        "# Create the new DataFrame\n",
        "result = (processed_df_1.groupby(['Member Id', 'Channel'])\n",
        "          .size()\n",
        "          .to_frame(name='count')\n",
        "          .reset_index()\n",
        "          .merge(processed_df_1[['Member Id', 'Channel', 'Payment Date']], how='left', on=['Member Id', 'Channel'])\n",
        "          .groupby(['Member Id', 'Channel'])['Payment Date']\n",
        "          .agg(lambda x: ', '.join(x.dt.strftime('%d %b %Y')))\n",
        "          .reset_index())\n",
        "\n",
        "# Display the result\n",
        "print(result)\n",
        "\n",
        "#this will create a table where i can see each member id from each channel adn there payments date\n",
        "#id channel payment_date\n",
        "#001  XYZ   21 jan 2022, 21 jan 2023, 21 jan 2024\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4LmH3ztSl_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'result' is the DataFrame containing the grouped data as shown in your example\n",
        "\n",
        "# Split the rows with multiple payment dates into separate rows\n",
        "result_expanded = result.assign(Payment_Date=result['Payment Date'].str.split(', ')).explode('Payment_Date')\n",
        "\n",
        "# Display the expanded result\n",
        "print(result_expanded[['Member Id', 'Channel', 'Payment_Date']])\n",
        "\n",
        "\n",
        "#now we will exploed"
      ],
      "metadata": {
        "id": "9cZ3-PVWSmBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'result_expanded' is the DataFrame containing the expanded data after exploding the rows\n",
        "\n",
        "# Count the occurrences of each Member Id\n",
        "member_id_counts = result_expanded['Member Id'].value_counts()\n",
        "\n",
        "# Filter Member Ids that have repeated more than once\n",
        "repeated_member_ids = member_id_counts[member_id_counts > 1].index\n",
        "\n",
        "# Filter and store the information of repeating Member Ids into temp_data\n",
        "temp_data = result_expanded[result_expanded['Member Id'].isin(repeated_member_ids)]\n",
        "\n",
        "# Display temp_data\n",
        "print(temp_data)\n",
        "\n",
        "\n",
        "#now we will have the users who are repeating here and there payment dates here also\n"
      ],
      "metadata": {
        "id": "5hhxXXqCT9iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'temp_data' is the DataFrame containing the relevant data\n",
        "\n",
        "# Convert 'Payment_Date' column to datetime\n",
        "temp_data['Payment_Date'] = pd.to_datetime(temp_data['Payment_Date'])\n",
        "\n",
        "# Calculate the time intervals between purchases for each Member Id\n",
        "temp_data['Time_Interval'] = temp_data.groupby('Member Id')['Payment_Date'].diff().dt.days\n",
        "\n",
        "# Categorize the time intervals into different categories\n",
        "conditions = [\n",
        "    temp_data['Time_Interval'].isnull(),  # First purchase (NaN)\n",
        "    temp_data['Time_Interval'] < 5,\n",
        "    (temp_data['Time_Interval'] >= 5) & (temp_data['Time_Interval'] <= 30),\n",
        "    (temp_data['Time_Interval'] > 30) & (temp_data['Time_Interval'] <= 90),\n",
        "    (temp_data['Time_Interval'] > 90) & (temp_data['Time_Interval'] <= 200),\n",
        "    (temp_data['Time_Interval'] > 200) & (temp_data['Time_Interval'] <= 365),\n",
        "    temp_data['Time_Interval'] > 365\n",
        "]\n",
        "\n",
        "categories = ['First Purchase', 'Less than 5 days', '5-30 days', '31-90 days', '91-200 days', '201-365 days', 'More than 365 days']\n",
        "\n",
        "# Create a new column 'Time_Category' based on the time interval categories\n",
        "temp_data['Time_Category'] = np.select(conditions, categories, default='Unknown')\n",
        "\n",
        "# Count the number of occurrences for each time interval category\n",
        "time_interval_counts = temp_data.groupby('Time_Category').size()\n",
        "\n",
        "# Display the counts for each time interval category\n",
        "for category, count in time_interval_counts.items():\n",
        "    print(f\"{category} - {count}\")\n",
        "\n",
        "\n",
        "#now we will be having the users here under certain condition\n"
      ],
      "metadata": {
        "id": "ylp1ZIM4T9nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "conditions are\n",
        "201-365 days - 388748\n",
        "31-90 days - 39453\n",
        "5-30 days - 32157\n",
        "91-200 days - 131405\n",
        "First Purchase - 3202493\n",
        "Less than 5 days - 4061519\n",
        "More than 365 days - 708576"
      ],
      "metadata": {
        "id": "xMXhDihnWJ17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#following the same conditions we will be seeing how conditions are met seprate for each channel\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'temp_data' is the DataFrame containing the relevant data\n",
        "\n",
        "# Convert 'Payment_Date' column to datetime\n",
        "temp_data['Payment_Date'] = pd.to_datetime(temp_data['Payment_Date'])\n",
        "\n",
        "# Calculate the time intervals between purchases for each Member Id within each Channel\n",
        "temp_data['Time_Interval'] = temp_data.groupby(['Channel', 'Member Id'])['Payment_Date'].diff().dt.days\n",
        "\n",
        "# Categorize the time intervals into different categories for each Channel\n",
        "conditions = [\n",
        "    temp_data['Time_Interval'].isnull(),  # First purchase (NaN)\n",
        "    temp_data['Time_Interval'] < 5,\n",
        "    (temp_data['Time_Interval'] >= 5) & (temp_data['Time_Interval'] <= 30),\n",
        "    (temp_data['Time_Interval'] > 30) & (temp_data['Time_Interval'] <= 90),\n",
        "    (temp_data['Time_Interval'] > 90) & (temp_data['Time_Interval'] <= 200),\n",
        "    (temp_data['Time_Interval'] > 200) & (temp_data['Time_Interval'] <= 365),\n",
        "    temp_data['Time_Interval'] > 365\n",
        "]\n",
        "\n",
        "categories = ['First Purchase', 'Less than 5 days', '5-30 days', '31-90 days', '91-200 days', '201-365 days', 'More than 365 days']\n",
        "\n",
        "# Create a new column 'Time_Category' based on the time interval categories\n",
        "temp_data['Time_Category'] = np.select(conditions, categories, default='Unknown')\n",
        "\n",
        "# Count the number of occurrences for each time interval category for each Channel\n",
        "time_interval_counts_by_channel = temp_data.groupby(['Channel', 'Time_Category']).size()\n",
        "\n",
        "# Display the counts for each time interval category for each Channel\n",
        "print(time_interval_counts_by_channel)\n"
      ],
      "metadata": {
        "id": "jkbdw4FiT9p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to see how many users are coming for the first time here\n",
        "temp_data.drop(columns=['Payment Date'], inplace=True)\n",
        "\n",
        "# Display the updated temp_data after dropping the 'Payment Date' column\n",
        "print(temp_data)\n"
      ],
      "metadata": {
        "id": "3Ige9GEHSmD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgplit3dWitV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCc_SDanWiv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oH6AH-TWiyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}